# ğŸ­ EEG-Based Emotion Recognition

## ğŸ“‘ Table of Contents
- [ğŸ“Œ Overview](#overview)
- [ğŸ“‚ Dataset Details](#dataset-details)
  - [ğŸ“ Online Ratings](#online-ratings)
  - [ğŸ§‘â€ğŸ”¬ Participant Ratings](#participant-ratings)
  - [ğŸ“¼ Video List](#video-list)
  - [ğŸ“Š Participant Questionnaire](#participant-questionnaire)
- [âš™ï¸ Data Preprocessing](#data-preprocessing)
- [ğŸ—‚ï¸ Dataset Structure](#dataset-structure)
- [ğŸ§  Feature Extraction](#feature-extraction)
- [ğŸ“ˆ Machine Learning Models](#machine-learning-models)
- [ğŸ“Š Accuracy Results](#accuracy-results)
- [ğŸš€ How to Use](#how-to-use)
- [ğŸ”® Future Improvements](#future-improvements)
- [ğŸ“§ Contact](#contact)

---

## ğŸ“Œ Overview
This project investigates **EEG-based emotion recognition** using **video stimuli**, both with and without **olfactory enhancement**. The goal is to evaluate the **impact of scent on emotion induction** and **recognition accuracy**, improving affective human-computer interaction (HCI). 

### ğŸ” Why is this Important?
- Enhancing **affective computing** to better understand human emotions.
- Improving **human-computer interaction** by incorporating brainwave data.
- Exploring **multisensory integration** to study the effect of smell in emotional experiences.

---

## ğŸ“‚ Dataset Details
This study uses EEG data collected from an emotion recognition experiment where participants were exposed to video-based stimuli.

### ğŸ“ Online Ratings
- Contains **emotional ratings** (Valence, Arousal, Dominance) provided by participants during an online study.
- Helps analyze the subjective emotional experience of participants.

### ğŸ§‘â€ğŸ”¬ Participant Ratings
- **Trial-by-trial emotional responses** recorded from participants while watching specific videos.
- Essential for linking EEG signals to emotional states.

### ğŸ“¼ Video List
- A comprehensive list of all **videos used in the self-assessment and experiment**.
- Helps in correlating emotional responses with specific video stimuli.

### ğŸ“Š Participant Questionnaire
- **Demographic information** and participant background details.
- Ensures diversity and reliability in data collection.

ğŸ”— **Dataset Source**: *[Provide dataset link if applicable]*

---

## âš™ï¸ Data Preprocessing
To ensure data quality and improve model performance, several preprocessing steps were performed:

- ğŸ”½ **Downsampling to 128Hz**: Reducing data size while preserving EEG frequency bands.
- ğŸ‘€ **EOG Artifact Removal**: Eliminating eye movement noise for better signal clarity.
- ğŸ“¶ **Bandpass Filtering (4-45 Hz)**: Keeping only emotion-related EEG wave frequencies.
- ğŸ”„ **Common Reference Averaging**: Normalizing signals across participants for consistency.
- ğŸ¯ **Trial Segmentation & Baseline Removal**: Ensuring data captures only emotion-induced responses.
- ğŸ†” **Reordering to Match Experiment ID**: Aligning dataset structure across participants.

---

## ğŸ—‚ï¸ Dataset Structure
- **ğŸ“Š Data Array (40x40x8064)**:
  - 40 trials per participant.
  - 40 EEG + physiological channels.
  - 8064 data points per trial (60 sec @ 128Hz).
- **ğŸ·ï¸ Labels (40x4)**:
  - Emotional dimensions: **Valence, Arousal, Dominance, Liking**.

---

## ğŸ§  Feature Extraction
### ğŸ”¬ EEG Features:
- **Power Bands**: Delta, Theta, Alpha, Beta, Gamma.
- **Differential Entropy (DE)**: Extracted from EEG signals.
- **Statistical Measures**: Mean, Variance, Skewness, Kurtosis.

### â¤ï¸ Emotional Labels:
- **Collected from participant ratings** in the experiment.

### ğŸ“¡ Physiological Data:
- **EOG and other biosignals** recorded before artifact removal.

### ğŸ“½ï¸ Video Metadata:
- **Contextual data**, such as video valence, arousal, and dominance scores.

---

## ğŸ“ˆ Machine Learning Models
We implemented the following **machine learning models** to classify emotions based on EEG data:

- ğŸ¤– **K-Nearest Neighbors (KNN)**
- ğŸ† **Support Vector Machine (SVM)**
- ğŸŒ³ **Random Forest**
- ğŸ“Š **Decision Tree**

---

## ğŸ“Š Accuracy Results
| ğŸ† Model | ğŸ¯ Train Accuracy | ğŸ“Š Test Accuracy |
|--------|--------------|-------------|
| **KNN** | 71.6% / 72.9% | 60.0% / 58.6% |
| **SVM** | 98.5% / 99.1% | 55.9% / 55.9% |
| **Random Forest** | 58.3% / 100% | 55.3% / 62.5% |
| **Decision Tree** | 99.3% / 57.4% | 60.0% / 58.2% |

---

## ğŸš€ How to Use
Follow these steps to use the project:

1. **Clone the Repository**:
   ```sh
   git clone https://github.com/Mansi111000/EEG-Based-Emotion-Recognition.git
   cd EEG-Based-Emotion-Recognition
   ```
2. **Run Preprocessing and Feature Extraction Scripts**.
3. **Train the models** using the provided Jupyter notebook or Python scripts.

---

## ğŸ”® Future Improvements
- ğŸ¤– **Deep Learning Integration**: Implementing CNNs and LSTMs for better feature extraction.
- ğŸ§  **Real-Time EEG Processing**: Developing a real-time emotion recognition system.
- ğŸ”— **Multimodal Analysis**: Combining EEG data with facial expression or speech recognition.
- ğŸ“ˆ **Dataset Expansion**: Collecting more diverse data for better model generalization.

---

## ğŸ“§ Contact
For any queries, reach out via **GitHub Issues** or email.

ğŸš€ **Happy Coding & Research!**
